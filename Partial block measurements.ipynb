{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partial block measurements\n",
    "\n",
    "This work is tracked in [T209403](https://phabricator.wikimedia.org/T209403).\n",
    "\n",
    "Originally, these measurements are geared towards being run on a monthly basis, using the Data Lake as the source. Currently, partial blocks are deployed on a low number of wikis and have not been deployed for long, meaning that monthly measurements makes little sense. Secondly, data on partial blocks are not yet available in the Data Lake.\n",
    "\n",
    "Until further notice, we'll grab the data from the replicated MediaWiki databases and explore whether daily measurements make sense.\n",
    "\n",
    "We have the following measurements defined in our master document, defined on a monthly basis:\n",
    "\n",
    "* Number of partial blocks for users/IPs where the block has a start timestamp within the month of interest.\n",
    "* Number of partial blocks for users/IPs where the duration of the partial block intersects with the month of interest and the user/IP makes ≥1 non-reverted edits during the intersection.\n",
    "* Number of partial blocks of users/IPs where:\n",
    "   * The partial block has a duration intersecting with the month of interest, and the block does not have pages added during the intersection.\n",
    "   * The partial block has a duration intersecting with the month of interest, and the block is not replaced by a sitewide block during the intersection.\n",
    "   * The partial block has an expiry timestamp within the month of interest and the expiration timestamp is not altered to increase the duration of the partial block.\n",
    "\n",
    "The first part of this notebook explores the first measurement, making plots of number of partial blocks set (for both registered users and IPs) per day for all wikis. Once we have data for partial blocks in the Data Lake, we'll turn these into monthly metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Python libraries\n",
    "\n",
    "import re\n",
    "import datetime as dt\n",
    "\n",
    "import pandas as pd\n",
    "import phpserialize as ps\n",
    "import pymysql\n",
    "\n",
    "from wmfdata import hive\n",
    "from growth import db, utils\n",
    "\n",
    "import mwreverts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the RPython library so we can use R for graphs\n",
    "\n",
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nettrom/venv/lib/python3.5/site-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: data.table 1.11.8  Latest news: r-datatable.com\n",
      "\n",
      "  warnings.warn(x, RRuntimeWarning)\n",
      "/home/nettrom/venv/lib/python3.5/site-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: \n",
      "Attaching package: ‘zoo’\n",
      "\n",
      "\n",
      "  warnings.warn(x, RRuntimeWarning)\n",
      "/home/nettrom/venv/lib/python3.5/site-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: The following objects are masked from ‘package:base’:\n",
      "\n",
      "    as.Date, as.Date.numeric\n",
      "\n",
      "\n",
      "  warnings.warn(x, RRuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "library(ggplot2);\n",
    "library(data.table);\n",
    "library(zoo);\n",
    "library(tidyr);\n",
    "library(RColorBrewer);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Some configuration variables.\n",
    "## Start and end timestamps allow us to speed up database queries.\n",
    "\n",
    "start_time = dt.datetime(2019, 1, 1, 0, 0, 0)\n",
    "end_time = dt.datetime(2019, 6, 14, 0, 0, 0)\n",
    "\n",
    "## Mapping from wiki DB name to host/port information\n",
    "dbhost_map = dict()\n",
    "\n",
    "## Mapping from wiki DB name to database connection\n",
    "dbconn_map = dict()\n",
    "\n",
    "## Format strings:\n",
    "## MediaWiki database timestamp format\n",
    "mw_format = \"%Y%m%d%H%M%S\"\n",
    "hive_format = \"%Y-%m-%dT%H:%M:%S\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "## See T218626 for the status of deployment of partial blocks\n",
    "\n",
    "wikis = ['arwiki', 'metawiki', 'mediawikiwiki', 'itwiki', 'fawiki',\n",
    "         'plwiki', 'frwiki', 'zhwiki', 'tewiki', 'bnwiki',\n",
    "         'huwiki', 'jawiki', 'hewiki', 'srwiki']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Some configuration:\n",
    "\n",
    "## Dates that we will use to make plots, they'll show from start date and up to,\n",
    "## but not including, the end date\n",
    "start_display_date = dt.date(2019, 1, 1)\n",
    "end_display_date = dt.date(2019, 6, 1)\n",
    "\n",
    "## We'll gather data from a year prior to the start date so we can have a yearly running\n",
    "## average across the whole graph\n",
    "start_data_date = dt.date(start_display_date.year - 1,\n",
    "                          start_display_date.month,\n",
    "                          start_display_date.day)\n",
    "end_data_date = end_display_date\n",
    "\n",
    "## Some statistics require a 28-day period, so in those cases we'll truncate to the previous\n",
    "## month, which we can get at with some date math (assuming that end_display_date is the\n",
    "## first of the month):\n",
    "truncated_end_date = (end_display_date - dt.timedelta(days = 1)).replace(day = 1)\n",
    "\n",
    "## The name of the database snapshot we're using, which should be the year and month\n",
    "## of the truncated end date\n",
    "snapshot = \"{}\".format(truncated_end_date.strftime(\"%Y-%m\"))\n",
    "\n",
    "## Because rpy2 doesn't convert datetime.date objects, we'll make the display dates\n",
    "## strings to help with that:\n",
    "start_display_date_str = str(start_display_date)\n",
    "end_display_date_str = str(end_display_date)\n",
    "truncated_end_date_str = str(truncated_end_date)\n",
    "\n",
    "## Filenames/paths to datasets that can be shared\n",
    "partial_monthly_filename = \"public_datasets/partial_blocks_monthly.tsv\"\n",
    "partial_constructive_filename = \"public_datasets/partial_blocks_constructive_edits.tsv\"\n",
    "partial_expired_filename = \"public_datasets/sitewide_expired_blocks.tsv\"\n",
    "partial_expired_noreblock_filename = \"public_datasets/sitewide_expired_not-reblocked.tsv\"\n",
    "partial_expired_nonreverted_filename = \"public_datasets/sitewide_expired_non-reverted.tsv\"\n",
    "partial_expired_nonreverted_ns0_filename = \"public_datasets/sitewide_expired_non-reverted_ns0.tsv\"\n",
    "\n",
    "def to_tsv(df, output_file):\n",
    "    df.to_csv(output_file, sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The second function needs dnspython to work\n",
    "import dns.resolver\n",
    "import glob\n",
    "\n",
    "def get_mediawiki_section_dbname_mapping(mw_config_path, use_x1):\n",
    "    db_mapping = {}\n",
    "    if use_x1:\n",
    "        dblist_section_paths = [mw_config_path.rstrip('/') + '/dblists/all.dblist']\n",
    "    else:\n",
    "        dblist_section_paths = glob.glob(mw_config_path.rstrip('/') + '/dblists/s[0-9]*.dblist')\n",
    "    for dblist_section_path in dblist_section_paths:\n",
    "        with open(dblist_section_path, 'r') as f:\n",
    "            for db in f.readlines():\n",
    "                db_mapping[db.strip()] = dblist_section_path.strip().rstrip('.dblist').split('/')[-1]\n",
    "\n",
    "    return db_mapping\n",
    "\n",
    "\n",
    "def get_dbstore_host_port(db_mapping, use_x1, dbname):\n",
    "    if dbname == 'staging':\n",
    "        shard = 'staging'\n",
    "    elif use_x1:\n",
    "        shard = 'x1'\n",
    "    else:\n",
    "        try:\n",
    "            shard = db_mapping[dbname]\n",
    "        except KeyError:\n",
    "            raise RuntimeError(\"The database {} is not listed among the dblist files of the supported sections.\"\n",
    "                               .format(dbname))\n",
    "    answers = dns.resolver.query('_' + shard + '-analytics._tcp.eqiad.wmnet', 'SRV')\n",
    "    host, port = str(answers[0].target), answers[0].port\n",
    "    return (host,port)\n",
    "\n",
    "wikidb_map = get_mediawiki_section_dbname_mapping('/srv/mediawiki-config', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "for wiki in wikis:\n",
    "    dbhost_map[wiki] = get_dbstore_host_port(wikidb_map, False, wiki)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "for wiki in wikis:\n",
    "    dbconn_map[wiki] = db.get_db_conn(wiki)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code form wmfdata to decode bytestrings returned from the database into UTF-8 strings\n",
    "\n",
    "def try_decode(cell):\n",
    "    try:\n",
    "        return cell.decode(encoding = \"utf-8\")\n",
    "    except AttributeError:\n",
    "        return cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_params(row):\n",
    "    '''\n",
    "    Extract relevant block parameters from the given row of partial block log\n",
    "    data, and return a new `pandas.Series` that can be used to update a data frame\n",
    "    with columns for those parameters.\n",
    "    '''\n",
    "    params = ps.loads(row['log_params'].encode('utf-8'), decode_strings=True)\n",
    "    \n",
    "    try:\n",
    "        duration = params['5::duration']\n",
    "    except KeyError:\n",
    "        duration = None\n",
    "        \n",
    "    try:\n",
    "        flags = params['6::flags']\n",
    "    except KeyError:\n",
    "        flags = None\n",
    "        \n",
    "    ## FIXME: investigate whether this needs changing, restrictions\n",
    "    ## might be a dict with keys \"pages\" or \"namespaces\", depending\n",
    "    ## on the block.\n",
    "    try:\n",
    "        num_pages = len(params['7::restrictions'])\n",
    "    except KeyError:\n",
    "        num_pages = None\n",
    "    \n",
    "    return(pd.Series([duration, flags, num_pages]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SQL query to get data on partial blocks, adapted from\n",
    "## https://github.com/dayllanmaza/wikireplicas-reports/blob/master/generators/partial_blocks.py\n",
    "\n",
    "def get_partial_blocks(wikis, dbconns, start_timestamp, end_timestamp):\n",
    "    pb_query = '''\n",
    "    SELECT DATABASE() AS wiki,\n",
    "           log_timestamp,\n",
    "           log_params,\n",
    "           log_user_text AS blocker,\n",
    "           log_title AS blockee,\n",
    "           comment_text AS reason\n",
    "    FROM {wiki}.logging\n",
    "    LEFT JOIN {wiki}.comment\n",
    "    ON log_comment_id=comment_id\n",
    "    WHERE log_timestamp >= \"{start_timestamp}\"\n",
    "    AND log_timestamp < \"{end_timestamp}\"\n",
    "    AND log_type = \"block\"\n",
    "    AND log_action = \"block\" -- only interested in initial blocks created\n",
    "    AND log_params LIKE '%\"sitewide\";b:0;%'\n",
    "    '''\n",
    "    \n",
    "    pbs = []\n",
    "    for wiki in wikis:\n",
    "        print('getting partial blocks from {}'.format(wiki))\n",
    "        df = pd.read_sql_query(\n",
    "            pb_query.format(\n",
    "                wiki = wiki,\n",
    "                start_timestamp = start_timestamp.strftime(mw_format),\n",
    "                end_timestamp = end_timestamp.strftime(mw_format)),\n",
    "            dbconns[wiki])\n",
    "        df = df.applymap(try_decode).rename(columns = try_decode)\n",
    "        \n",
    "        if len(df) == 0:\n",
    "            continue\n",
    "        \n",
    "        ## Turn the timestamps into datetime objects, and add a log_date string for convenience\n",
    "        df['log_timestamp'] = pd.to_datetime(df['log_timestamp'], format=mw_format, utc=True)\n",
    "        df['log_date'] = df['log_timestamp'].apply(lambda x: str(x.date()))\n",
    "        \n",
    "        df[['block_duration', 'block_flags', 'block_num_pages']] = df.apply(extract_params, axis=1)\n",
    "        \n",
    "        pbs.append(df)\n",
    "    \n",
    "    return(pd.concat(pbs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting partial blocks from arwiki\n",
      "getting partial blocks from metawiki\n",
      "getting partial blocks from mediawikiwiki\n",
      "getting partial blocks from itwiki\n",
      "getting partial blocks from fawiki\n",
      "getting partial blocks from plwiki\n",
      "getting partial blocks from frwiki\n",
      "getting partial blocks from zhwiki\n",
      "getting partial blocks from tewiki\n",
      "getting partial blocks from bnwiki\n",
      "getting partial blocks from huwiki\n",
      "getting partial blocks from jawiki\n",
      "getting partial blocks from hewiki\n",
      "getting partial blocks from srwiki\n"
     ]
    }
   ],
   "source": [
    "partial_blocks = get_partial_blocks(wikis, dbconn_map, start_time, end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nettrom/venv/lib/python3.5/site-packages/rpy2/robjects/pandas2ri.py:191: FutureWarning: from_items is deprecated. Please use DataFrame.from_dict(dict(items), ...) instead. DataFrame.from_dict(OrderedDict(items)) may be used to preserve the key order.\n",
      "  res = PandasDataFrame.from_items(items)\n",
      "/home/nettrom/venv/lib/python3.5/site-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: geom_path: Each group consists of only one observation. Do you need to adjust\n",
      "the group aesthetic?\n",
      "\n",
      "  warnings.warn(x, RRuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "%%R -i partial_blocks\n",
    "\n",
    "partial_blocks = data.table(partial_blocks);\n",
    "partial_blocks[, log_date := as.Date(log_date)];\n",
    "\n",
    "## Some configuration variables\n",
    "graph_dir = 'graphs/';\n",
    "pb_graph_prefix = 'partial_blocks_per_day_';\n",
    "pb_graph_suffix = '.png';\n",
    "\n",
    "make_pb_graphs = function(pbs, graph_dir, prefix, suffix) {\n",
    "    wikis = unique(pbs$wiki);\n",
    "    \n",
    "    for(w in wikis) {\n",
    "        ## Grab the subset for this wiki, as we have different date ranges for each\n",
    "        wiki_blocks = pbs[wiki == w];\n",
    "        \n",
    "        ## Make a date sequence from the first to the last date, and left join against\n",
    "        ## the data to fill in any dates with 0 blocks.\n",
    "\n",
    "        dates = seq.Date(min(wiki_blocks$log_date), max(wiki_blocks$log_date), by='day');\n",
    "        dates = data.table(log_date = dates);\n",
    "\n",
    "        blocks_per_day = wiki_blocks[, list(num_blocks=sum(.N)), by=log_date];\n",
    "        blocks_per_day = blocks_per_day[dates, on = 'log_date'];\n",
    "        blocks_per_day[is.na(num_blocks), num_blocks := 0];\n",
    "\n",
    "        ## Add the moving averages\n",
    "        blocks_per_day[\n",
    "            , num_blocks_1wma := rollapply(\n",
    "                num_blocks,\n",
    "                width = 7,\n",
    "                FUN = mean,\n",
    "                na.rm = TRUE,\n",
    "                fill = 0,\n",
    "                align = 'right')];\n",
    "        blocks_per_day[\n",
    "            , num_blocks_2wma := rollapply(\n",
    "                num_blocks,\n",
    "                width = 14,\n",
    "                FUN = mean,\n",
    "                na.rm = TRUE,\n",
    "                fill = 0,\n",
    "                align = 'right')];\n",
    "        \n",
    "        ## Tidy up and make the plot\n",
    "        blocks_per_day_long = blocks_per_day %>% gather(measure, num_blocks, 2:4);\n",
    "        blocks_per_day_long = data.table(blocks_per_day_long);\n",
    "        blocks_per_day_long[measure == 'num_blocks', measure := 'raw data'];\n",
    "        blocks_per_day_long[measure == 'num_blocks_1wma', measure := '1-week MA'];\n",
    "        blocks_per_day_long[measure == 'num_blocks_2wma', measure := '2-week MA'];\n",
    "        blocks_per_day_long[\n",
    "            , measure := ordered(measure, rev(c('raw data', '1-week MA', '2-week MA')))];\n",
    "\n",
    "        ## Choose blues with some contrast, with the raw data getting the strongest color\n",
    "        b_palette = brewer.pal('Blues', n = 7)[c(3,5,7)];\n",
    "        \n",
    "        block_day_plot = ggplot(blocks_per_day_long,\n",
    "                                aes(x=log_date, y=num_blocks, color=measure)) +\n",
    "        scale_x_date(date_breaks = \"1 week\", date_labels = \"%d %b\") +\n",
    "        scale_y_continuous() +\n",
    "        scale_colour_manual(values = b_palette) +\n",
    "        expand_limits(y = 0) +\n",
    "        labs(title = paste0('Partial blocks created per day - ', w),\n",
    "             x = 'Date',\n",
    "             y = 'Number of blocks') +\n",
    "        theme_light(base_size = 14) +\n",
    "        geom_line();\n",
    "\n",
    "        ggsave(paste0(graph_dir, prefix, w, suffix),\n",
    "           plot = block_day_plot, width = 30, height = 20, units = \"cm\", dpi = \"screen\");\n",
    "    }\n",
    "}\n",
    "\n",
    "make_pb_graphs(partial_blocks, graph_dir, pb_graph_prefix, pb_graph_suffix);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partial blocks per month\n",
    "\n",
    "\"Number of partial blocks for users/IPs where the block has a start timestamp within the month of interest.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "pb_monthly_query = '''\n",
    "SELECT pbs.wiki_db,\n",
    "       DATE_FORMAT(pbs.start_timestamp, 'yyyy-MM-01') AS log_month,\n",
    "       count(*) AS num_blocks\n",
    "FROM (\n",
    "    SELECT wiki_db,\n",
    "    start_timestamp,\n",
    "    COALESCE(LAG(inferred_from, 1) OVER (PARTITION BY wiki_db, user_id ORDER BY start_timestamp),\n",
    "        \"block\") AS prev_inferred,\n",
    "    LAG(caused_by_event_type, 1) OVER (PARTITION BY wiki_db, user_id ORDER BY start_timestamp)\n",
    "        AS prev_event_type,\n",
    "    LAG(end_timestamp, 1) OVER (PARTITION BY wiki_db, user_id ORDER BY start_timestamp)\n",
    "        AS prev_end_timestamp\n",
    "    FROM wmf.mediawiki_user_history\n",
    "    WHERE snapshot = \"{snapshot}\"\n",
    "      AND wiki_db IN ({wikis})\n",
    "      AND start_timestamp BETWEEN \"{start_date}\" AND \"{end_date}\"\n",
    "      AND caused_by_event_type = 'alterblocks'\n",
    "      AND inferred_from IS NULL -- not an unblock\n",
    "      AND source_log_params['sitewide'] = 'false' -- don't forget that's a string, not boolean!\n",
    "    ) AS pbs\n",
    "WHERE pbs.prev_end_timestamp IS NULL\n",
    "   OR pbs.prev_inferred IS NULL\n",
    "   OR NOT (-- prev event is also a block, meaning this is a reblock\n",
    "    pbs.prev_end_timestamp = pbs.start_timestamp\n",
    "    AND pbs.prev_inferred = \"block\"\n",
    "    AND pbs.prev_event_type = 'alterblocks')\n",
    "GROUP BY pbs.wiki_db,\n",
    "DATE_FORMAT(pbs.start_timestamp, 'yyyy-MM-01')\n",
    "LIMIT 10000\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_counts = hive.run(pb_monthly_query.format(\n",
    "    snapshot = snapshot,\n",
    "    start_date = start_data_date,\n",
    "    end_date = end_data_date,\n",
    "    wikis = \",\".join(['\"{}\"'.format(w) for w in wikis])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I've verified that the monthly counts (197 as of the April 2019 snapshot) matches the actual data, as there were 220 blocks in total and 23 of them were reblocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nettrom/venv/lib/python3.5/site-packages/rpy2/robjects/pandas2ri.py:191: FutureWarning: from_items is deprecated. Please use DataFrame.from_dict(dict(items), ...) instead. DataFrame.from_dict(OrderedDict(items)) may be used to preserve the key order.\n",
      "  res = PandasDataFrame.from_items(items)\n",
      "/home/nettrom/venv/lib/python3.5/site-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: geom_path: Each group consists of only one observation. Do you need to adjust\n",
      "the group aesthetic?\n",
      "\n",
      "  warnings.warn(x, RRuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "%%R -i monthly_counts,start_display_date_str,end_display_date_str\n",
    "\n",
    "## Some configuration variables\n",
    "graph_dir = 'graphs/';\n",
    "pb_monthly_prefix = 'partial_blocks_per_month_';\n",
    "pb_graph_suffix = '.png';\n",
    "\n",
    "wiki_names = list(\n",
    "    arwiki = 'Arabic Wikipedia',\n",
    "    metawiki = 'Meta Wiki',\n",
    "    mediawikiwiki = 'MediaWiki Wiki',\n",
    "    itwiki = 'Italian Wikipedia',\n",
    "    fawiki = 'Persian Wikipedia',\n",
    "    plwiki = 'Polish Wikipedia',\n",
    "    frwiki = 'French Wikipedia',\n",
    "    zhwiki = 'Chinese Wikipedia',\n",
    "    tewiki = 'Telugu Wikipedia',\n",
    "    bnwiki = 'Bengali Wikipedia',\n",
    "    huwiki = 'Hungarian Wikipedia',\n",
    "    jawiki = 'Japanese Wikipedia',\n",
    "    hewiki = 'Hebrew Wikipedia',\n",
    "    srwiki = 'Serbian Wikipedia');\n",
    "\n",
    "monthly_counts = data.table(monthly_counts);\n",
    "monthly_counts[, log_month := as.Date(log_month)];\n",
    "\n",
    "## Add 3-month and 12-month running average\n",
    "monthly_counts[\n",
    "    , num_blocks_3mma := rollapply(\n",
    "        num_blocks,\n",
    "        width = 3,\n",
    "        FUN = mean,\n",
    "        na.rm = TRUE,\n",
    "        fill = 0,\n",
    "        align = 'right')];\n",
    "monthly_counts[\n",
    "    , num_blocks_1yma := rollapply(\n",
    "        num_blocks,\n",
    "        width = 12,\n",
    "        FUN = mean,\n",
    "        na.rm = TRUE,\n",
    "        fill = 0,\n",
    "        align = 'right')];\n",
    "\n",
    "monthly_counts_long = monthly_counts %>% gather(measure, num_blocks, 3:5);\n",
    "monthly_counts_long = data.table(monthly_counts_long);\n",
    "monthly_counts_long[measure == 'num_blocks', measure := 'raw data'];\n",
    "monthly_counts_long[measure == 'num_blocks_3mma', measure := '3-month MA'];\n",
    "monthly_counts_long[measure == 'num_blocks_1yma', measure := '1-year MA'];\n",
    "monthly_counts_long[\n",
    "    , measure := ordered(measure, rev(c('raw data', '3-month MA', '1-year MA')))];\n",
    "\n",
    "make_ma_plot = function(dt, wiki, start_date, end_date, title, breaks) {\n",
    "    b_palette = brewer.pal('Blues', n = 7)[c(3,5,7)];\n",
    "    ggplot(\n",
    "        dt[log_month >= start_date &\n",
    "           log_month < end_date &\n",
    "           wiki_db == wiki],\n",
    "        aes(x=log_month, y=num_blocks, colour = measure)) +\n",
    "    scale_x_date(date_breaks = \"1 month\", date_labels = \"%Y-%m\") +\n",
    "    scale_y_continuous(breaks = breaks) +\n",
    "    expand_limits(y = 0) +\n",
    "    scale_colour_manual(values = b_palette) +\n",
    "    labs(title = title,\n",
    "         x = 'Date',\n",
    "         y = 'Number of blocks') +\n",
    "    geom_line() +\n",
    "    theme_light(base_size = 14);\n",
    "}\n",
    "\n",
    "for(wiki in monthly_counts_long$wiki_db) {\n",
    "    g = make_ma_plot(monthly_counts_long, wiki, start_display_date_str, end_display_date_str,\n",
    "                     paste(\"Partial blocks created per month -\", wiki_names[[wiki]]), c(0:10*10));\n",
    "    ggsave(paste0(graph_dir, pb_monthly_prefix, wiki, pb_graph_suffix),\n",
    "           plot = g, width = 40, height = 20, units = \"cm\", dpi = \"screen\");\n",
    "};rm(wiki);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Export the partial blocks monthly count dataset\n",
    "\n",
    "to_tsv(monthly_counts, partial_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partially blocked user/IP makes constructive edits\n",
    "\n",
    "The second measurement we have defined is:\n",
    "\n",
    "* Number of partial blocks for users/IPs where the duration of the partial block intersects with the month of interest and the user/IP makes ≥1 non-reverted edits during the intersection.\n",
    "\n",
    "Because this is about block overlap with months, and collapsing edits during the overlap, it's probably easier to do this with programming than with SQL.\n",
    "\n",
    "**TODO:** Reconsider whether infinite blocks cannot be part of this analysis. In the sitewide block analysis, we disregard infinite blocks because they're mainly used to shut down vandalism-only accounts. A partial block is a different type of tool so counting them makes more sense. For example, they might be used to enforce an interaction ban. Also, I suspect they are somewhat rare.\n",
    "\n",
    "For now, I've removed the \"can't be an infinite block\" restriction as I don't think it makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "pb_constructive_query = '''\n",
    "SELECT pbs.wiki_db,  pbs.start_timestamp, pbs.end_timestamp,\n",
    "       revisions.event_user_id,\n",
    "       revisions.event_timestamp AS rev_timestamp\n",
    "FROM (\n",
    "    SELECT wiki_db, user_id, start_timestamp, end_timestamp\n",
    "    FROM (\n",
    "        SELECT wiki_db, user_id,\n",
    "        start_timestamp, end_timestamp,\n",
    "        COALESCE(LAG(inferred_from, 1) OVER (PARTITION BY wiki_db, user_id ORDER BY start_timestamp),\n",
    "            \"block\") AS prev_inferred,\n",
    "        LAG(caused_by_event_type, 1) OVER (PARTITION BY wiki_db, user_id ORDER BY start_timestamp)\n",
    "            AS prev_event_type,\n",
    "        LAG(end_timestamp, 1) OVER (PARTITION BY wiki_db, user_id ORDER BY start_timestamp)\n",
    "            AS prev_end_timestamp\n",
    "        FROM wmf.mediawiki_user_history\n",
    "        WHERE snapshot = \"{snapshot}\"\n",
    "          AND wiki_db IN ({wikis})\n",
    "          AND start_timestamp BETWEEN \"{start_date}\" AND \"{end_date}\"\n",
    "          AND caused_by_event_type = 'alterblocks'\n",
    "          AND inferred_from IS NULL -- not an unblock\n",
    "          AND source_log_params['sitewide'] = 'false' -- don't forget that's a string, not boolean!\n",
    "    ) AS a\n",
    "    WHERE a.prev_end_timestamp IS NULL\n",
    "       OR a.prev_inferred IS NULL\n",
    "       OR NOT (\n",
    "          -- prev event is also a block, meaning this is a reblock\n",
    "          a.prev_end_timestamp = a.start_timestamp\n",
    "          AND a.prev_inferred = \"block\"\n",
    "          AND a.prev_event_type = 'alterblocks')\n",
    ") AS pbs\n",
    "JOIN (\n",
    "    SELECT wiki_db, event_user_id, event_timestamp, event_entity\n",
    "    FROM wmf.mediawiki_history\n",
    "    WHERE snapshot = \"{snapshot}\"\n",
    "    AND wiki_db IN ({wikis})\n",
    "    AND event_entity = \"revision\"\n",
    "    AND event_type = \"create\"\n",
    "    AND event_timestamp > \"{start_date}\"\n",
    "    AND (revision_is_identity_reverted = FALSE\n",
    "         OR revision_seconds_to_identity_revert > 60*60*48)\n",
    ") AS revisions\n",
    "ON pbs.wiki_db = revisions.wiki_db\n",
    "   AND pbs.user_id = revisions.event_user_id\n",
    "WHERE revisions.event_timestamp BETWEEN pbs.start_timestamp AND pbs.end_timestamp\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "constructive_pb_edits = hive.run(pb_constructive_query.format(\n",
    "    snapshot = snapshot,\n",
    "    start_date = start_data_date,\n",
    "    end_date = end_data_date,\n",
    "    wikis = \",\".join(['\"{}\"'.format(w) for w in wikis])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_block_id(row):\n",
    "    return(str(row.event_user_id) + \\\n",
    "           dt.datetime.strptime(row.start_timestamp, '%Y-%m-%d %H:%M:%S.0').strftime('%Y%m%d%H%M%S') + \\\n",
    "           dt.datetime.strptime(row.end_timestamp, '%Y-%m-%d %H:%M:%S.0').strftime('%Y%m%d%H%M%S'))\n",
    "\n",
    "## TODO: does this support infinite blocks?\n",
    "def count_constructive(df, start_timestamp, end_timestamp):\n",
    "    '''\n",
    "    For the given `pandas.DataFrame` `df` containing constructive (non-reverted)\n",
    "    edits made by partially blocked users during their block, and for every month\n",
    "    between `start_timestamp` and `end_timestamp`, calculate the number of blocks\n",
    "    with constructive edits. Group these by wiki.\n",
    "    '''\n",
    "    \n",
    "    ## I need a way to identify each block so that we don't count blocks multiple times\n",
    "    ## during each month. To do that, I'll concatenate user ID, start, and end timestamps.\n",
    "    if not 'block_id' in df:\n",
    "        df['block_id'] = df.apply(make_block_id, axis = 1)\n",
    "        \n",
    "    if not 'rev_ts' in df:\n",
    "        df['rev_ts'] = pd.to_datetime(df['rev_timestamp'], format = \"%Y-%m-%d %H:%M:%S.0\")\n",
    "    \n",
    "    ## Then, iterate over wiki and month, identify all constructive edits within that month,\n",
    "    ## then collapse to count the number of unique block IDs.\n",
    "    date_range = pd.date_range(start_timestamp, end_timestamp, freq = \"MS\")\n",
    "    \n",
    "    res_df = pd.DataFrame(columns = ['wiki_db', 'log_month', 'num_blocks'])\n",
    "    \n",
    "    for wiki in df.wiki_db.unique():\n",
    "        for month_idx in range(len(date_range) -1): ## iterate but exclude the last month\n",
    "            n_blocks = df.loc[(df.wiki_db == wiki) &\n",
    "                              (df.rev_ts >= date_range[month_idx]) &\n",
    "                              (df.rev_ts < date_range[month_idx + 1])]['block_id'].nunique()\n",
    "            res_df = res_df.append(\n",
    "                pd.DataFrame([[wiki, date_range[month_idx], n_blocks]],\n",
    "                             columns = ['wiki_db', 'log_month', 'num_blocks']\n",
    "                )\n",
    "            )\n",
    "\n",
    "    res_df['num_blocks'] = res_df['num_blocks'].astype(int)\n",
    "            \n",
    "    return(res_df)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_blocks_with_constr_edits = count_constructive(constructive_pb_edits, start_time, end_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%R -i num_blocks_with_constr_edits,start_display_date_str,end_display_date_str\n",
    "\n",
    "pb_constructive_prefix = 'partial_blocks_constructive_per_month_';\n",
    "\n",
    "num_blocks_with_constr_edits = data.table(num_blocks_with_constr_edits);\n",
    "num_blocks_with_constr_edits[, log_month := as.Date(log_month)];\n",
    "\n",
    "## Add 3-month and 12-month running average\n",
    "num_blocks_with_constr_edits[\n",
    "    , num_blocks_3mma := rollapply(\n",
    "        num_blocks,\n",
    "        width = 3,\n",
    "        FUN = mean,\n",
    "        na.rm = TRUE,\n",
    "        fill = 0,\n",
    "        align = 'right')];\n",
    "num_blocks_with_constr_edits[\n",
    "    , num_blocks_1yma := rollapply(\n",
    "        num_blocks,\n",
    "        width = 12,\n",
    "        FUN = mean,\n",
    "        na.rm = TRUE,\n",
    "        fill = 0,\n",
    "        align = 'right')];\n",
    "\n",
    "num_blocks_with_constr_edits_long = num_blocks_with_constr_edits %>% gather(measure, num_blocks, 3:5);\n",
    "num_blocks_with_constr_edits_long = data.table(num_blocks_with_constr_edits_long);\n",
    "num_blocks_with_constr_edits_long[measure == 'num_blocks', measure := 'raw data'];\n",
    "num_blocks_with_constr_edits_long[measure == 'num_blocks_3mma', measure := '3-month MA'];\n",
    "num_blocks_with_constr_edits_long[measure == 'num_blocks_1yma', measure := '1-year MA'];\n",
    "num_blocks_with_constr_edits_long[\n",
    "    , measure := ordered(measure, rev(c('raw data', '3-month MA', '1-year MA')))];\n",
    "\n",
    "for(wiki in num_blocks_with_constr_edits_long$wiki_db) {\n",
    "    g = make_ma_plot(num_blocks_with_constr_edits_long, wiki, start_display_date_str, end_display_date_str,\n",
    "                     paste(\"Partial blocks w/constructive edits per month -\", wiki_names[[wiki]]),\n",
    "                     c(0:20*5));\n",
    "    ggsave(paste0(graph_dir, pb_constructive_prefix, wiki, pb_graph_suffix),\n",
    "           plot = g, width = 40, height = 20, units = \"cm\", dpi = \"screen\");\n",
    "};rm(wiki);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Export the constructive edits monthly dataset\n",
    "\n",
    "to_tsv(num_blocks_with_constr_edits, partial_constructive_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial blocks that are not altered\n",
    "\n",
    "The last part of our measurements look at whether partial blocks are altered, either by having pages added, being replaced with a sitewide block, or by having the duration increased. Note that these go together with a logical \"or\", so we're reporting only one measurement:\n",
    "\n",
    "* Number of partial blocks of users/IPs where:\n",
    "  * The partial block has a duration intersecting with the month of interest, and the block does not have pages added during the intersection.\n",
    "  * The partial block has a duration intersecting with the month of interest, and the block is not replaced by a sitewide block during the intersection.\n",
    "  * The partial block has an expiry timestamp within the month of interest and the expiration timestamp is not altered to increase the duration of the partial block.\n",
    "  \n",
    "### Notes\n",
    "\n",
    "After digging deeper into the data, it looks like the restrictions are stored as a string-representation of a Map, rather than an actual Map. This means that I might want to grab all partial blocks and enough information to answer all three questions, and write Python to do the analysis.\n",
    "\n",
    "The information I need for that is:\n",
    "* Value of restrictions for this block and the next block\n",
    "* Value of sitewide for this block and next block (current block has to be false, next block has to be true)\n",
    "* Value of the expiration date for this block and the next block.\n",
    "\n",
    "Regarding expiry timestamp: `caused_by_block_expiration` is set to \"infinite\" for indef blocks, and it's set to a numeric value if the block had an expiry timestamp set (note that it looks like MediaWiki operates with 30-day months for simplicity). So if the next event is a reblock and the expiry timestamp is extended, we can exclude it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This query retrieves all the necessary information to calculate\n",
    "## our final measurement.\n",
    "\n",
    "pb_candidates_query = '''\n",
    "SELECT wiki_db, user_id, start_timestamp, end_timestamp, source_log_params, next_params,\n",
    "       caused_by_block_expiration, next_expiration\n",
    "FROM (\n",
    "    SELECT wiki_db, user_id,\n",
    "    start_timestamp, end_timestamp, caused_by_block_expiration,\n",
    "    COALESCE(LAG(inferred_from, 1) OVER (PARTITION BY wiki_db, user_id ORDER BY start_timestamp),\n",
    "        \"block\") AS prev_inferred,\n",
    "    LAG(caused_by_event_type, 1) OVER (PARTITION BY wiki_db, user_id ORDER BY start_timestamp)\n",
    "        AS prev_event_type,\n",
    "    LAG(end_timestamp, 1) OVER (PARTITION BY wiki_db, user_id ORDER BY start_timestamp)\n",
    "        AS prev_end_timestamp,\n",
    "    LEAD(start_timestamp, 1) OVER (PARTITION BY wiki_db, user_id ORDER BY start_timestamp)\n",
    "        AS next_start_timestamp,\n",
    "    LEAD(caused_by_event_type, 1) OVER (PARTITION BY wiki_db, user_id ORDER BY start_timestamp)\n",
    "        AS next_event_type,\n",
    "    COALESCE(LEAD(inferred_from, 1) OVER (PARTITION BY wiki_db, user_id ORDER BY start_timestamp),\n",
    "        \"block\") AS next_inferred,\n",
    "    LEAD(source_log_params, 1) OVER (PARTITION BY wiki_db, user_id ORDER BY start_timestamp)\n",
    "        AS next_params,\n",
    "    LEAD(caused_by_block_expiration, 1) OVER (PARTITION BY wiki_db, user_id ORDER BY start_timestamp)\n",
    "        AS next_expiration\n",
    "    FROM wmf.mediawiki_user_history\n",
    "    WHERE snapshot = \"{snapshot}\"\n",
    "      AND wiki_db IN ({wikis})\n",
    "      AND start_timestamp BETWEEN \"{start_date}\" AND \"{end_date}\"\n",
    "      AND caused_by_event_type = \"alterblocks\"\n",
    "      AND inferred_from IS NULL -- not an unblock\n",
    "      AND source_log_params[\"sitewide\"] = \"false\" -- don't forget that's a string, not boolean!\n",
    ") AS a\n",
    "WHERE a.prev_end_timestamp IS NULL\n",
    "   OR a.prev_inferred IS NULL\n",
    "   OR NOT (\n",
    "      -- prev event is also a block, meaning this is a reblock\n",
    "      a.prev_end_timestamp = a.start_timestamp\n",
    "      AND a.prev_inferred = \"block\"\n",
    "      AND a.prev_event_type = \"alterblocks\"))\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pb_candidates = hive.run(pb_candidates_query.format(\n",
    "    \n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_nonmodified(df, start_timestamp, end_timestamp):\n",
    "    '''\n",
    "    For the given `pandas.DataFrame` `df` containing information on initial partial\n",
    "    blocks, identify blocks that were in effect during a given month and remove those\n",
    "    that meet either of the following criteria:\n",
    "    \n",
    "    * Had pages or namespaces added to it.\n",
    "    * Was replaced with a sitewide block.\n",
    "    * Had its duration extended.\n",
    "    '''\n",
    "    \n",
    "    ## Iterate over wiki and month, and for each month identify all\n",
    "    ## partial blocks in effect that month that do not meet either\n",
    "    ## of the exclusion criteria.\n",
    "    date_range = pd.date_range(start_timestamp, end_timestamp, freq = \"MS\")\n",
    "    \n",
    "    res_df = pd.DataFrame(columns = ['wiki', 'log_month', 'n_blocks'])\n",
    "    \n",
    "    for wiki in df.wiki_db.unique():\n",
    "        for month_idx in range(len(date_range) -1): ## iterate but exclude the last month\n",
    "            candidates = df.loc[(df.wiki_db == wiki) &\n",
    "                                (df.start_timestamp < date_range[month_idx + 1]) &\n",
    "                                (df.end_timestamp > date_range[month_idx])]\n",
    "            \n",
    "            ## Exclude blocks that had pages or namespaces added to them\n",
    "            ## (this boils down to counting the number of \"->\" in the restrictions)\n",
    "            candidates = candidates.loc[(candidates.next_params.isna()) |\n",
    "                                        (candidates.source_log_params['7::restrictions'].map(lambda x: x.count('->')) <\n",
    "                                         candidates.next_params['7::restrictions'].map(lambda x: x.count('->')))]\n",
    "            \n",
    "            ## Exclude blocks that were replaced with a sitewide block\n",
    "            candidates = candidates.loc[(candidates.next_params.isna()) |\n",
    "                                        ((candidates.source_log_params['sitewide'] == False) &\n",
    "                                         (candidates.next_params['sitewide'] == True))]\n",
    "            \n",
    "            ## Exclude blocks that had their duration extended\n",
    "            \n",
    "            ## Add the wiki, month, and number of non-modified blocks to the results.\n",
    "            res_df = res_df.append(\n",
    "                pd.DataFrame([[wiki, date_range[month_idx], len(candidates)]],\n",
    "                             columns = ['wiki', 'log_month', 'n_blocks']\n",
    "                )\n",
    "            )\n",
    "\n",
    "    return(res_df)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I looked through the partial blocks on Italian Wikipedia, as that is the wiki where blocks have been used most. There, I find that alterations of the partial blocks happens very rarely. While I didn't go through all of them carefully, I found only one block where this happened. That block was altered two times, one to change the duration from \"infinite\" to \"1 year\" and at the same time add one page to the block list, then again to add another page (but keep the duration)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
