{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partial block measurements\n",
    "\n",
    "This work is tracked in [T209403](https://phabricator.wikimedia.org/T209403).\n",
    "\n",
    "Originally, these measurements are geared towards being run on a monthly basis, using the Data Lake as the source. Currently, partial blocks are deployed on a low number of wikis and have not been deployed for long, meaning that monthly measurements makes little sense. Secondly, data on partial blocks are not yet available in the Data Lake.\n",
    "\n",
    "Until further notice, we'll grab the data from the replicated MediaWiki databases and explore whether daily measurements make sense.\n",
    "\n",
    "We have the following measurements defined in our master document, defined on a monthly basis:\n",
    "\n",
    "* Number of partial blocks for users/IPs where the block has a start timestamp within the month of interest.\n",
    "* Number of partial blocks for users/IPs where the duration of the partial block intersects with the month of interest and the user/IP makes ≥1 non-reverted edits during the intersection.\n",
    "* Number of partial blocks of users/IPs where:\n",
    "   * The partial block has a duration intersecting with the month of interest, and the block does not have pages added during the intersection.\n",
    "   * The partial block has a duration intersecting with the month of interest, and the block is not replaced by a sitewide block during the intersection.\n",
    "   * The partial block has an expiry timestamp within the month of interest and the expiration timestamp is not altered to increase the duration of the partial block.\n",
    "\n",
    "The first part of this notebook explores the first measurement, making plots of number of partial blocks set (for both registered users and IPs) per day for all wikis. Once we have data for partial blocks in the Data Lake, we'll turn these into monthly metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Python libraries\n",
    "\n",
    "import pymysql\n",
    "\n",
    "import datetime as dt\n",
    "import pandas as pd\n",
    "\n",
    "import phpserialize as ps\n",
    "\n",
    "from wmfdata import hive\n",
    "\n",
    "import mwreverts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load the RPython library so we can use R for graphs\n",
    "\n",
    "%load_ext rpy2.ipython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nettrom/venv/lib/python3.5/site-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: data.table 1.11.8  Latest news: r-datatable.com\n",
      "\n",
      "  warnings.warn(x, RRuntimeWarning)\n",
      "/home/nettrom/venv/lib/python3.5/site-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: \n",
      "Attaching package: ‘zoo’\n",
      "\n",
      "\n",
      "  warnings.warn(x, RRuntimeWarning)\n",
      "/home/nettrom/venv/lib/python3.5/site-packages/rpy2/rinterface/__init__.py:146: RRuntimeWarning: The following objects are masked from ‘package:base’:\n",
      "\n",
      "    as.Date, as.Date.numeric\n",
      "\n",
      "\n",
      "  warnings.warn(x, RRuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "library(ggplot2);\n",
    "library(data.table);\n",
    "library(zoo);\n",
    "library(tidyr);\n",
    "library(RColorBrewer);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Some configuration variables.\n",
    "## Start and end timestamps allow us to speed up database queries.\n",
    "\n",
    "start_time = dt.datetime(2019, 1, 1, 0, 0, 0)\n",
    "end_time = dt.datetime(2019, 4, 23, 0, 0, 0)\n",
    "\n",
    "## The wikis that we are interested in studying\n",
    "wikis = ['itwiki', 'arwiki', 'fawiki', 'metawiki', 'mediawikiwiki']\n",
    "\n",
    "## Mapping from wiki DB name to host/port information\n",
    "dbhost_map = dict()\n",
    "\n",
    "## Mapping from wiki DB name to database connection\n",
    "dbconn_map = dict()\n",
    "\n",
    "## Format strings:\n",
    "## MediaWiki database timestamp format\n",
    "mw_format = \"%Y%m%d%H%M%S\"\n",
    "hive_format = \"%Y-%m-%dT%H:%M:%S\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The second function needs dnspython to work\n",
    "import dns.resolver\n",
    "import glob\n",
    "\n",
    "def get_mediawiki_section_dbname_mapping(mw_config_path, use_x1):\n",
    "    db_mapping = {}\n",
    "    if use_x1:\n",
    "        dblist_section_paths = [mw_config_path.rstrip('/') + '/dblists/all.dblist']\n",
    "    else:\n",
    "        dblist_section_paths = glob.glob(mw_config_path.rstrip('/') + '/dblists/s[0-9]*.dblist')\n",
    "    for dblist_section_path in dblist_section_paths:\n",
    "        with open(dblist_section_path, 'r') as f:\n",
    "            for db in f.readlines():\n",
    "                db_mapping[db.strip()] = dblist_section_path.strip().rstrip('.dblist').split('/')[-1]\n",
    "\n",
    "    return db_mapping\n",
    "\n",
    "\n",
    "def get_dbstore_host_port(db_mapping, use_x1, dbname):\n",
    "    if dbname == 'staging':\n",
    "        shard = 'staging'\n",
    "    elif use_x1:\n",
    "        shard = 'x1'\n",
    "    else:\n",
    "        try:\n",
    "            shard = db_mapping[dbname]\n",
    "        except KeyError:\n",
    "            raise RuntimeError(\"The database {} is not listed among the dblist files of the supported sections.\"\n",
    "                               .format(dbname))\n",
    "    answers = dns.resolver.query('_' + shard + '-analytics._tcp.eqiad.wmnet', 'SRV')\n",
    "    host, port = str(answers[0].target), answers[0].port\n",
    "    return (host,port)\n",
    "\n",
    "wikidb_map = get_mediawiki_section_dbname_mapping('/srv/mediawiki-config', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "for wiki in wikis:\n",
    "    dbhost_map[wiki] = get_dbstore_host_port(wikidb_map, False, wiki)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "for wiki in wikis:\n",
    "    dbconn_map[wiki] = pymysql.connect(\n",
    "        host = dbhost_map[wiki][0],\n",
    "        port = dbhost_map[wiki][1],\n",
    "        database = wiki,\n",
    "        read_default_file = '/etc/mysql/conf.d/research-client.cnf',\n",
    "        charset = 'utf8'\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Code form wmfdata to decode bytestrings returned from the database into UTF-8 strings\n",
    "\n",
    "def try_decode(cell):\n",
    "    try:\n",
    "        return cell.decode(encoding = \"utf-8\")\n",
    "    except AttributeError:\n",
    "        return cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_params(row):\n",
    "    '''\n",
    "    Extract relevant block parameters from the given row of partial block log\n",
    "    data, and return a new `pandas.Series` that can be used to update a data frame\n",
    "    with columns for those parameters.\n",
    "    '''\n",
    "    params = ps.loads(row['log_params'].encode('utf-8'), decode_strings=True)\n",
    "    \n",
    "    try:\n",
    "        duration = params['5::duration']\n",
    "    except KeyError:\n",
    "        duration = None\n",
    "        \n",
    "    try:\n",
    "        flags = params['6::flags']\n",
    "    except KeyError:\n",
    "        flags = None\n",
    "        \n",
    "    try:\n",
    "        num_pages = len(params['7::restrictions'])\n",
    "    except KeyError:\n",
    "        num_pages = None\n",
    "    \n",
    "    return(pd.Series([duration, flags, num_pages]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SQL query to get data on partial blocks, adapted from\n",
    "## https://github.com/dayllanmaza/wikireplicas-reports/blob/master/generators/partial_blocks.py\n",
    "\n",
    "def get_partial_blocks(wikis, dbconns, start_timestamp, end_timestamp):\n",
    "    pb_query = '''\n",
    "    SELECT DATABASE() AS wiki,\n",
    "           log_timestamp,\n",
    "           log_params,\n",
    "           log_user_text AS blocker,\n",
    "           log_title AS blockee,\n",
    "           comment_text AS reason\n",
    "    FROM {wiki}.logging\n",
    "    LEFT JOIN {wiki}.comment\n",
    "    ON log_comment_id=comment_id\n",
    "    WHERE log_timestamp >= \"{start_timestamp}\"\n",
    "    AND log_timestamp < \"{end_timestamp}\"\n",
    "    AND log_type = \"block\"\n",
    "    AND log_action = \"block\" -- only interested in initial blocks created\n",
    "    AND log_params LIKE '%\"sitewide\";b:0;%'\n",
    "    '''\n",
    "    \n",
    "    pbs = []\n",
    "    for wiki in wikis:\n",
    "        df = pd.read_sql_query(\n",
    "            pb_query.format(\n",
    "                wiki = wiki,\n",
    "                start_timestamp = start_timestamp.strftime(mw_format),\n",
    "                end_timestamp = end_timestamp.strftime(mw_format)),\n",
    "            dbconns[wiki])\n",
    "        df = df.applymap(try_decode).rename(columns = try_decode)\n",
    "        \n",
    "        ## Turn the timestamps into datetime objects, and add a log_date string for convenience\n",
    "        df['log_timestamp'] = pd.to_datetime(df['log_timestamp'], format=mw_format, utc=True)\n",
    "        df['log_date'] = df['log_timestamp'].apply(lambda x: str(x.date()))\n",
    "\n",
    "        df[['block_duration', 'block_flags', 'block_num_pages']] = df.apply(extract_params, axis=1)\n",
    "        \n",
    "        pbs.append(df)\n",
    "    \n",
    "    return(pd.concat(pbs))\n",
    "\n",
    "partial_blocks = get_partial_blocks(wikis, dbconn_map, start_time, end_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>wiki</th>\n",
       "      <th>log_timestamp</th>\n",
       "      <th>log_params</th>\n",
       "      <th>blocker</th>\n",
       "      <th>blockee</th>\n",
       "      <th>reason</th>\n",
       "      <th>log_date</th>\n",
       "      <th>block_duration</th>\n",
       "      <th>block_flags</th>\n",
       "      <th>block_num_pages</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>itwiki</td>\n",
       "      <td>2019-01-16 12:15:11+00:00</td>\n",
       "      <td>a:4:{s:11:\"5::duration\";s:8:\"infinite\";s:8:\"6:...</td>\n",
       "      <td>Daimona Eaytoy</td>\n",
       "      <td>Eaytoy_Daimona</td>\n",
       "      <td>Test blocchi parziali</td>\n",
       "      <td>2019-01-16</td>\n",
       "      <td>infinite</td>\n",
       "      <td>nocreate</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>itwiki</td>\n",
       "      <td>2019-01-16 12:36:56+00:00</td>\n",
       "      <td>a:4:{s:11:\"5::duration\";s:8:\"infinite\";s:8:\"6:...</td>\n",
       "      <td>Daimona Eaytoy</td>\n",
       "      <td>Eaytoy_Daimona</td>\n",
       "      <td></td>\n",
       "      <td>2019-01-16</td>\n",
       "      <td>infinite</td>\n",
       "      <td>nocreate</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>itwiki</td>\n",
       "      <td>2019-01-16 12:52:53+00:00</td>\n",
       "      <td>a:4:{s:11:\"5::duration\";s:7:\"2 hours\";s:8:\"6::...</td>\n",
       "      <td>Ruthven</td>\n",
       "      <td>151.20.139.29</td>\n",
       "      <td>[[WP:Vandalismo|Vandalismi]]</td>\n",
       "      <td>2019-01-16</td>\n",
       "      <td>2 hours</td>\n",
       "      <td>anononly,nocreate</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>itwiki</td>\n",
       "      <td>2019-01-16 13:01:55+00:00</td>\n",
       "      <td>a:4:{s:11:\"5::duration\";s:7:\"8 hours\";s:8:\"6::...</td>\n",
       "      <td>Ruthven</td>\n",
       "      <td>93.55.168.167</td>\n",
       "      <td>[[WP:Vandalismo|Vandalismi]]</td>\n",
       "      <td>2019-01-16</td>\n",
       "      <td>8 hours</td>\n",
       "      <td>anononly,nocreate</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>itwiki</td>\n",
       "      <td>2019-01-16 13:26:10+00:00</td>\n",
       "      <td>a:3:{s:11:\"5::duration\";s:9:\"5 minutes\";s:8:\"6...</td>\n",
       "      <td>Buggia</td>\n",
       "      <td>Wolframio</td>\n",
       "      <td></td>\n",
       "      <td>2019-01-16</td>\n",
       "      <td>5 minutes</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     wiki             log_timestamp  \\\n",
       "0  itwiki 2019-01-16 12:15:11+00:00   \n",
       "1  itwiki 2019-01-16 12:36:56+00:00   \n",
       "2  itwiki 2019-01-16 12:52:53+00:00   \n",
       "3  itwiki 2019-01-16 13:01:55+00:00   \n",
       "4  itwiki 2019-01-16 13:26:10+00:00   \n",
       "\n",
       "                                          log_params         blocker  \\\n",
       "0  a:4:{s:11:\"5::duration\";s:8:\"infinite\";s:8:\"6:...  Daimona Eaytoy   \n",
       "1  a:4:{s:11:\"5::duration\";s:8:\"infinite\";s:8:\"6:...  Daimona Eaytoy   \n",
       "2  a:4:{s:11:\"5::duration\";s:7:\"2 hours\";s:8:\"6::...         Ruthven   \n",
       "3  a:4:{s:11:\"5::duration\";s:7:\"8 hours\";s:8:\"6::...         Ruthven   \n",
       "4  a:3:{s:11:\"5::duration\";s:9:\"5 minutes\";s:8:\"6...          Buggia   \n",
       "\n",
       "          blockee                        reason    log_date block_duration  \\\n",
       "0  Eaytoy_Daimona         Test blocchi parziali  2019-01-16       infinite   \n",
       "1  Eaytoy_Daimona                                2019-01-16       infinite   \n",
       "2   151.20.139.29  [[WP:Vandalismo|Vandalismi]]  2019-01-16        2 hours   \n",
       "3   93.55.168.167  [[WP:Vandalismo|Vandalismi]]  2019-01-16        8 hours   \n",
       "4       Wolframio                                2019-01-16      5 minutes   \n",
       "\n",
       "         block_flags  block_num_pages  \n",
       "0           nocreate              1.0  \n",
       "1           nocreate              1.0  \n",
       "2  anononly,nocreate              1.0  \n",
       "3  anononly,nocreate              1.0  \n",
       "4                                 NaN  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partial_blocks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nettrom/venv/lib/python3.5/site-packages/rpy2/robjects/pandas2ri.py:191: FutureWarning: from_items is deprecated. Please use DataFrame.from_dict(dict(items), ...) instead. DataFrame.from_dict(OrderedDict(items)) may be used to preserve the key order.\n",
      "  res = PandasDataFrame.from_items(items)\n"
     ]
    }
   ],
   "source": [
    "%%R -i partial_blocks\n",
    "\n",
    "partial_blocks = data.table(partial_blocks);\n",
    "partial_blocks[, log_date := as.Date(log_date)];\n",
    "\n",
    "## Some configuration variables\n",
    "graph_dir = 'graphs/';\n",
    "pb_graph_prefix = 'partial_blocks_per_day_';\n",
    "pb_graph_suffix = '.png';\n",
    "\n",
    "make_pb_graphs = function(pbs, graph_dir, prefix, suffix) {\n",
    "    wikis = unique(pbs$wiki);\n",
    "    \n",
    "    for(w in wikis) {\n",
    "        ## Grab the subset for this wiki, as we have different date ranges for each\n",
    "        wiki_blocks = pbs[wiki == w];\n",
    "        \n",
    "        ## Make a date sequence from the first to the last date, and left join against\n",
    "        ## the data to fill in any dates with 0 blocks.\n",
    "\n",
    "        dates = seq.Date(min(wiki_blocks$log_date), max(wiki_blocks$log_date), by='day');\n",
    "        dates = data.table(log_date = dates);\n",
    "\n",
    "        blocks_per_day = wiki_blocks[, list(num_blocks=sum(.N)), by=log_date];\n",
    "        blocks_per_day = blocks_per_day[dates, on = 'log_date'];\n",
    "        blocks_per_day[is.na(num_blocks), num_blocks := 0];\n",
    "\n",
    "        ## Add the moving averages\n",
    "        blocks_per_day[\n",
    "            , num_blocks_1wma := rollapply(\n",
    "                num_blocks,\n",
    "                width = 7,\n",
    "                FUN = mean,\n",
    "                na.rm = TRUE,\n",
    "                fill = 0,\n",
    "                align = 'right')];\n",
    "        blocks_per_day[\n",
    "            , num_blocks_2wma := rollapply(\n",
    "                num_blocks,\n",
    "                width = 14,\n",
    "                FUN = mean,\n",
    "                na.rm = TRUE,\n",
    "                fill = 0,\n",
    "                align = 'right')];\n",
    "        \n",
    "        ## Tidy up and make the plot\n",
    "        blocks_per_day_long = blocks_per_day %>% gather(measure, num_blocks, 2:4);\n",
    "        blocks_per_day_long = data.table(blocks_per_day_long);\n",
    "        blocks_per_day_long[measure == 'num_blocks', measure := 'raw data'];\n",
    "        blocks_per_day_long[measure == 'num_blocks_1wma', measure := '1-week MA'];\n",
    "        blocks_per_day_long[measure == 'num_blocks_2wma', measure := '2-week MA'];\n",
    "        blocks_per_day_long[\n",
    "            , measure := ordered(measure, rev(c('raw data', '1-week MA', '2-week MA')))];\n",
    "\n",
    "        ## Choose blues with some contrast, with the raw data getting the strongest color\n",
    "        b_palette = brewer.pal('Blues', n = 7)[c(3,5,7)];\n",
    "        \n",
    "        block_day_plot = ggplot(blocks_per_day_long,\n",
    "                                aes(x=log_date, y=num_blocks, color=measure)) +\n",
    "        scale_x_date(date_breaks = \"1 week\", date_labels = \"%d %b\") +\n",
    "        scale_y_continuous() +\n",
    "        scale_colour_manual(values = b_palette) +\n",
    "        expand_limits(y = 0) +\n",
    "        labs(title = paste0('Partial blocks created per day - ', w),\n",
    "             x = 'Date',\n",
    "             y = 'Number of blocks') +\n",
    "        theme_light(base_size = 14) +\n",
    "        geom_line();\n",
    "\n",
    "        ggsave(paste0(graph_dir, prefix, w, suffix),\n",
    "           plot = block_day_plot, width = 30, height = 20, units = \"cm\", dpi = \"screen\");\n",
    "    }\n",
    "}\n",
    "\n",
    "make_pb_graphs(partial_blocks, graph_dir, pb_graph_prefix, pb_graph_suffix);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partially blocked user/IP makes constructive edits\n",
    "\n",
    "The second measurement we have defined is:\n",
    "\n",
    "* Number of partial blocks for users/IPs where the duration of the partial block intersects with the month of interest and the user/IP makes ≥1 non-reverted edits during the intersection.\n",
    "\n",
    "At this point, we do not have monthly data, so we'll instead look at the daily number of active partial blocks where the blocked user/IP makes ≥1 non-reverted edits that day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "def was_reverted(db_conn, rev_id, rev_page, rev_timestamp, rev_sha1,\n",
    "                 is_archive = False, radius = 15, hours = 48):\n",
    "    '''\n",
    "    Use the mwreverts library to determine if the given edit is reverted within `radius`\n",
    "    number of edits or `hours` number of hours. `is_archive` means the archive table will\n",
    "    be used to identify the edit.\n",
    "    '''\n",
    "    \n",
    "    ## We'll have to grab radius - 1 no. of edits from before the given edit,\n",
    "    ## then radius no. of edits (within the given time) from after the given edit\n",
    "    ## and then search through those.\n",
    "    \n",
    "    table_name = 'revision'\n",
    "    id_column = 'rev_id'\n",
    "    ts_column = 'rev_timestamp'\n",
    "    sha1_column = 'rev_sha1'\n",
    "    page_column = 'rev_page'\n",
    "    if is_archive:\n",
    "        table_name = 'archive'\n",
    "        id_column = 'ar_rev_id'\n",
    "        ts_column = 'ar_timestamp'\n",
    "        sha1_column = 'ar_sha1'\n",
    "        page_column = 'ar_page_id'\n",
    "        \n",
    "    pre_query = '''\n",
    "    SELECT rev_id, rev_timestamp, rev_sha1\n",
    "    FROM (\n",
    "        SELECT {id_col} AS rev_id, {ts_col} AS rev_timestamp,\n",
    "               {sha1_col} AS rev_sha1\n",
    "        FROM {table}\n",
    "        WHERE {page_col} = {rev_page}\n",
    "        AND {ts_col} <= \"{rev_timestamp}\"\n",
    "        ORDER BY {ts_col} DESC\n",
    "        LIMIT {radius}\n",
    "    ) AS pr\n",
    "    ORDER BY rev_timestamp ASC'''\n",
    "    \n",
    "    post_query = '''\n",
    "    SELECT {id_col} AS rev_id, {ts_col} AS rev_timestamp,\n",
    "           {sha1_col} AS rev_sha1\n",
    "    FROM {table}\n",
    "    WHERE {page_col} = {rev_page}\n",
    "    AND {ts_col} > \"{rev_timestamp}\"\n",
    "    AND {ts_col} < \"{max_timestamp}\"\n",
    "    ORDER BY {ts_col} ASC\n",
    "    LIMIT {radius}'''\n",
    "    \n",
    "    max_timestamp = (dt.datetime.strptime(rev_timestamp, mw_format)\n",
    "                     + dt.timedelta(hours = 48)).strftime(mw_format)\n",
    "    \n",
    "    rev_detector = mwreverts.Detector()\n",
    "    \n",
    "    was_reverted = False\n",
    "    \n",
    "    with db_conn.cursor() as db_cursor:\n",
    "        db_cursor.execute(pre_query.format(\n",
    "            table = table_name, id_col = id_column, ts_col = ts_column,\n",
    "            sha1_col = sha1_column, page_col = page_column, rev_page = rev_page,\n",
    "            rev_timestamp = rev_timestamp, radius = radius))\n",
    "        for row in db_cursor:\n",
    "            r_id = row[0]\n",
    "            r_timestamp = row[1].decode('utf-8')\n",
    "            r_sha1 = row[2].decode('utf-8')\n",
    "            \n",
    "            rev_detector.process(r_sha1, {'rev_id': r_id,\n",
    "                                            'rev_timestamp': r_timestamp})\n",
    "        \n",
    "        db_cursor.execute(post_query.format(\n",
    "            table = table_name, id_col = id_column, ts_col = ts_column,\n",
    "            sha1_col = sha1_column, page_col = page_column, rev_page = rev_page,\n",
    "            rev_timestamp = rev_timestamp, max_timestamp = max_timestamp,\n",
    "            radius = radius))\n",
    "        for row in db_cursor:\n",
    "            r_id = row[0]\n",
    "            r_timestamp = row[1].decode('utf-8')\n",
    "            r_sha1 = row[2].decode('utf-8')\n",
    "            \n",
    "            revert = rev_detector.process(r_sha1, {'rev_id': r_id,\n",
    "                                                     'rev_timestamp': r_timestamp})\n",
    "            \n",
    "            if revert:\n",
    "                for revision in revert.reverteds:\n",
    "                    if revision['rev_id'] == rev_id:\n",
    "                        was_reverted = True\n",
    "                        \n",
    "    return(was_reverted)\n",
    "\n",
    "def test_revert():\n",
    "    enwiki = get_dbstore_host_port(wikidb_map, False, 'enwiki')\n",
    "    en_conn = pymysql.connect(\n",
    "        host = enwiki[0],\n",
    "        port = enwiki[1],\n",
    "        database = 'enwiki',\n",
    "        read_default_file = '/etc/mysql/conf.d/research-client.cnf',\n",
    "        charset = 'utf8'\n",
    "    )\n",
    "    print(was_reverted(en_conn, 794446073, 24911978,\n",
    "                       '20170808022628', 'mp9mjock323d1tfi5likm1yf0lfa9j5'))\n",
    "    print(was_reverted(en_conn, 793784102, 24911978,\n",
    "                       '20170803225004', 'qw89z7d27jodyb26v35fmj33hirhko1'))\n",
    "    en_conn.close()\n",
    "    \n",
    "test_revert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Note: we'll have to grab log data for _all_ partial blocks, so we know when they start,\n",
    "## when they expire, and if they had any reblock/unblock events affecting their duration.\n",
    "## This also means we'll need to figure out when they expire #sadface\n",
    "\n",
    "## For each wiki...\n",
    "##   For each block...\n",
    "##     Grab any user edits during that timespan (from both revision and archive tables).\n",
    "##     Investigate if either of those edits were reverted within k edits or 48 hours.\n",
    "##     For any edits that were not reverted, increase the count for that day.\n",
    "\n",
    "## So, we're looking at having a data structure with counts for every day\n",
    "## (defaultdict of ints with day as the key).\n",
    "## At the end, turn that into a pandas.DataFrame for the days and counts,\n",
    "## then graph that.\n",
    "\n",
    "## The range in the dataset needs to reflect the range in the block data. Meaning that\n",
    "## the first and last dates in this measurement should be the first and last days for which\n",
    "## we have at least one block set. \"last day\" will be the minimum of the configured end date\n",
    "## and the last day we have a block set for.\n",
    "\n",
    "def constructive_blocked_edits(df, start_timestamp, end_timestamp):\n",
    "    '''\n",
    "    For the given dataframe with data on partial blocks, calculate the number of blocks\n",
    "    for each day between start_timestamp and end_timestamp where the blocked user/IP\n",
    "    made at least one constructive edit.\n",
    "    '''\n",
    "    \n",
    "    ## actual start date is maximum of the start_timestamp and the first block in the dataset\n",
    "    ## actual last date is the minimum of the end_timestamp and the expiration of the last\n",
    "    ## block in the dataset\n",
    "    \n",
    "    return()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SELECT DATABASE() AS wiki,\n",
    "       log_action,\n",
    "       log_timestamp,\n",
    "       log_params,\n",
    "       log_user_text AS blocker,\n",
    "       log_title AS blockee,\n",
    "       comment_text AS reason\n",
    "FROM itwiki.logging\n",
    "LEFT JOIN itwiki.comment\n",
    "ON log_comment_id=comment_id\n",
    "WHERE log_timestamp >= \"20190101000000\"\n",
    "AND log_timestamp < \"20190327000000\"\n",
    "AND log_type = \"block\"\n",
    "AND log_params LIKE '%\"sitewide\";b:0;%';"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
